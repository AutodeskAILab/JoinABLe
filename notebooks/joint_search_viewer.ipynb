{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Search Viewer\n",
    "View the results of joint pose search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import meshplot as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "root_dir = Path().resolve().parent\n",
    "if str(root_dir) not in sys.path:\n",
    "    sys.path.append(str(root_dir))\n",
    "\n",
    "from utils import util\n",
    "from joint.joint_environment import JointEnvironment\n",
    "from joint.joint_prediction_set import JointPredictionSet\n",
    "\n",
    "from search.search_simplex import SearchSimplex\n",
    "from search.search_random import SearchRandom\n",
    "\n",
    "from train import JointPrediction\n",
    "from datasets.joint_graph_dataset import JointGraphDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Network & Data\n",
    "Load a pretrained checkpoint to use for inference.\n",
    "\n",
    "Load the dataset and create an instance of the JointPredictionSet class.\n",
    "We assume that the joint json and mesh part files are in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cache loaded from: /Users/willisk/Autodesk/Code/Github/JoinABLe/data/tester/val.pickle\n"
     ]
    }
   ],
   "source": [
    "def load_network(checkpoint_file):\n",
    "    \"\"\"Load the network\"\"\"\n",
    "    if not checkpoint_file.exists():\n",
    "        print(\"Checkpoint file does not exist\")\n",
    "        return None\n",
    "    model = JointPrediction.load_from_checkpoint(\n",
    "        checkpoint_file,\n",
    "        map_location=torch.device(\"cpu\")  # Just use the CPU\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Use a checkpoint from the pretrained model\n",
    "checkpoint_file = root_dir / \"pretrained/paper/last_run_0.ckpt\"\n",
    "model = load_network(checkpoint_file)\n",
    "\n",
    "# Change to point to the Fusion 360 Gallery joint dataset\n",
    "# this directory should contain the joint json and obj part files\n",
    "data_dir = root_dir / \"data/tester\"\n",
    "dataset = JointGraphDataset(\n",
    "    root_dir=data_dir,\n",
    "    split=\"val\",\n",
    "    label_scheme=\"Joint,JointEquivalent\"\n",
    ")\n",
    "# Data sample in the dataset we want to visualize\n",
    "index = 2\n",
    "# Graphs for part one and two, and the densely connected joint graph\n",
    "g1, g2, joint_graph = dataset[index]\n",
    "# The joint file json\n",
    "joint_file = data_dir / dataset.files[index]\n",
    "# Random seed to use\n",
    "seed = 24\n",
    "\n",
    "# Create the prediction with the given data and model\n",
    "jps = JointPredictionSet(\n",
    "    joint_file,\n",
    "    g1, g2, joint_graph,\n",
    "    model,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth\n",
    "View the ground truth assembled state of the parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc57f3e1a1c4756be070cc7626ee82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(18.776182…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v, f, c, e = jps.get_meshes(\n",
    "    joint_index=0,\n",
    "    show_joint_entity_colors=False,\n",
    "    show_joint_equivalent_colors=False,\n",
    ")\n",
    "p = mp.plot(v, f, c=c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Axis Prediction without Search\n",
    "View the parts when assembled using only the joint axis prediction, without performing search for the offset/rotation/flip parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a9c0ac637f407d8c956fc466da41f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(22.458543…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the transform to move body1, to align with the static body2\n",
    "# using default parameters, i.e. no offset, rotation, or flip\n",
    "transform = JointEnvironment.get_transform_from_parameters(\n",
    "    jps,\n",
    "    prediction_index=0,  # Top-1 prediction\n",
    "    offset=0,\n",
    "    rotation_in_degrees=0,\n",
    "    flip=False\n",
    ")\n",
    "\n",
    "# Render the meshes to visualize\n",
    "v, f, c, e, n, ni = jps.get_meshes(\n",
    "    apply_transform=True,\n",
    "    body_one_transform=transform,\n",
    "    show_joint_entity_colors=False,\n",
    "    show_joint_equivalent_colors=False,\n",
    "    return_vertex_normals=True\n",
    ")\n",
    "p = mp.plot(v, f, c=c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Pose Search\n",
    "Perform joint pose search to find the offset, rotation, and flip parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction_index': 37,\n",
       " 'offset': 2.615928649902344e-06,\n",
       " 'rotation': 0.00046525239944457977,\n",
       " 'flip': True,\n",
       " 'transform': array([[ 0.76620711,  0.        ,  0.6425937 ,  3.90841466],\n",
       "        [ 0.        , -1.        ,  0.        ,  0.00784779],\n",
       "        [-0.6425937 ,  0.        ,  0.76620711,  6.01150962],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ]]),\n",
       " 'evaluation': -1.102427577096359,\n",
       " 'overlap': 0.0021510395545753275,\n",
       " 'contact': 0.11045786166509343}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state = np.random.RandomState(seed)\n",
    "# Nelder–Mead Simplex Search as used in the paper\n",
    "search = SearchSimplex(random_state=random_state)\n",
    "# Random search can also be used\n",
    "# search = SearchRandom(random_state=random_state, budget=500)\n",
    "\n",
    "result = search.search(jps)\n",
    "# Returns a dict with:\n",
    "# - prediction_index: Index of the prediction from the network, 0 being the highest probability\n",
    "# - offset: Offset parameter\n",
    "# - rotation: Rotation parameter\n",
    "# - flip: Flip parameter\n",
    "# - transform: Transform created from the axis and parameters, apply this transform to body 2 will assemble the parts together\n",
    "# - evaluation: Evaluation score where lower is better\n",
    "# - overlap: Overlap between the parts\n",
    "# - contact: Contact between the parts\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf0c66f1f53401a945c707975f1516d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(9.9963521…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualized the \n",
    "v, f, c, e, n, ni = jps.get_meshes(\n",
    "    apply_transform=True,\n",
    "    body_one_transform=result[\"transform\"],\n",
    "    show_joint_entity_colors=False,\n",
    "    show_joint_equivalent_colors=False,\n",
    "    return_vertex_normals=True\n",
    ")\n",
    "p = mp.plot(v, f, c=c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4bf1c82db291c90cc4874b8785149ede8d32de9ceb77e89bfd1ded6bd42ea729"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
